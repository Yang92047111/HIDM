%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Formal Book Title Page
% LaTeX Template
% Version 2.0 (23/7/17)
%
% This template was downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Peter Wilson (herries.press@earthlink.net) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% This template can be used in one of two ways:
%
% 1) Content can be added at the end of this file just before the \end{document}
% to use this title page as the starting point for your document.
%
% 2) Alternatively, if you already have a document which you wish to add this
% title page to, copy everything between the \begin{document} and
% \end{document} and paste it where you would like the title page in your
% document. You will then need to insert the packages and document 
% configurations into your document carefully making sure you are not loading
% the same package twice and that there are no clashes.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt, oneside]{article} % A4 paper size, default 11pt font size and oneside for equal margins

\newcommand{\plogo}{\fbox{$\mathcal{PL}$}} % Generic dummy publisher logo

\usepackage{CJKutf8}
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{fouriernc} % Use the New Century Schoolbook font
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{enumerate}


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begin{document} 

\begin{titlepage} % Suppresses headers and footers on the title page

	\centering % Centre everything on the title page
	
	\scshape % Use small caps for all text on the title page
	
	\vspace*{\baselineskip} % White space at the top of the page

	\begin{figure}

        \centering

        \includegraphics[width=0.7\textwidth]{../output/logo}

    \end{figure}
	
	%------------------------------------------------
	%	Title
	%------------------------------------------------
	
	\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt} % Thick horizontal rule
	\rule{\textwidth}{0.4pt} % Thin horizontal rule
	
	\vspace{0.75\baselineskip} % Whitespace above the title
	
	{\LARGE Human Information Data Mining} % Title
	
	\vspace{0.75\baselineskip} % Whitespace below the title
	
	\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal rule
	\rule{\textwidth}{1.6pt} % Thick horizontal rule
	
	\vspace{2\baselineskip} % Whitespace after the title block
	
	%------------------------------------------------
	%	Subtitle
	%------------------------------------------------
	
	Homework 3 % Subtitle or further description
	
	\vspace*{3\baselineskip} % Whitespace under the subtitle
	
	%------------------------------------------------
	%	Editor(s)
	%------------------------------------------------
	
	Student ID:

	\vspace{0.5\baselineskip}

	{\scshape 108368017}

	\vspace{0.5\baselineskip}

	Student: 
	
	\vspace{0.5\baselineskip} % Whitespace before the editors

	% {\scshape 108368017 \hspace{10mm}}
	{\scshape\Large Zi-Yang Lin } % Editor list

	\vspace{0.5\baselineskip} % Whitespace before the editors


	Advisor:

	\vspace{0.5\baselineskip} % Whitespace before the editors
	
	{\scshape\Large Jenq-Haur Wang} % Editor list
	
	\vspace{0.5\baselineskip} % Whitespace below the editor list
	
	\textit{National Taipei University of Technology} % Editor affiliation
	
	\vfill % Whitespace between editor names and publisher logo
	
	%------------------------------------------------
	%	Publisher
	%------------------------------------------------
	
	% \plogo % Publisher logo
	
	\vspace{0.3\baselineskip} % Whitespace under the publisher logo
	
	2019 % Publication year
	
	% {\large publisher} % Publisher

\end{titlepage}

%----------------------------------------------------------------------------------------

\clearpage

\section*{Problem 1}
\textbf{8.6:} Why is Naïve Bayesian classification called ``naïve'' ? 
Briefly outline the ideas of Naïve Bayesian classification.
\subsection*{Ans:}

\textbf{Naïve Bayesian classification} is called ``naïve'' because it assume class conditional
independence. That is, the effect of an attribute value on a given class is independent of 
the values of the other attributes. The major idea behind ``naïve'' Bayesian classification is to 
try and classify data by maximizing $P(\textbf{X} \vert C_{i})P(C_{i})$ (where $i$ is an index of the class)
using the Bayes' theorem of posterior probability.

\begin{itemize}

	\item We are given a set of unknown data tuples, where each tuple is represented by 
	an $n$-dimensional vector, $X = (x_{1}, x_{2}, \ldots, x_{n})$ depicting n measurements 
	made on the tuple from n attributes, respectively $A_{1}, A_{2}, \ldots, A_{n}$. We are also given 
	a set of m classes, $C_{1}, C_{2}, \ldots, C_{m}$.

	\item Using Bayes theorem, the naïve Bayesian classifier calculates the posterior 
	probability of each class conditioned on $\textbf{X}$. $\textbf{X}$ is assigned 
	the class label of the class with the maximum posterior probability conditioned on 
	$\textbf{X}$. Therefore, we try to maximize $P(C_{i} \vert \textbf{X}) = P(X|Ci)P(Ci)/P(X)$.
	However, since $P(\textbf{X})$ is constant for all classes, only $P(\textbf{X} \vert C_{i})
	P(C_{i})$ need be maximized. If the class prior probabilities are not known, 
	then it is commonly assumed that the classes are equally likely, i.e. 
	$P(C_{1}) = P(C_{2}) = \ldots = P(C_{m})$, and we would therefore maximize $P(\textbf{X} \vert C_{i})$.
	Otherwise, we maximize $P(\textbf{X} \vert C_{i})P(C_{i})$. The class prior probabilities may be estimated by 
	$P(C_{i}) = \frac{s_{i}}{s}$,where $s_{i}$ is the number of training tuples of class $C_{i}$, 
	and $s$ is the total number of training tuples.

	\item In order to reduce computation in evaluating $P(\textbf{X} \vert C_{i})$,
	the naïve assumption of \textbf{class conditional independence} is made.
	This presumes that the values of the attributes are conditionally
	independent of one another, given the class label of the tuple, i.e., 
	that there are no dependence relationships among the attributes.

	\begin{itemize}
		
		\item If $A_{k}$ is a categorical attribute then $P(x_{k} \vert C_{i})$ is equal to the number of training tuples in $C_{i}$
		that have $x_{k}$ as the value for that attribute, divided by the total number of training tuples in $C_{i}$.

		\item If $A_{k}$ is a continuous attribute then $P(x_{k} \vert C_{i})$ can be calculated using a Gaussian density function.

	\end{itemize}

\end{itemize}


\section*{Problem 2}
\textbf{8.11:} The harmonic mean is one of several kinds of averages. 
Chapter 2 discussed how to compute the arithmetic mean, 
which is what most people typically think of when they compute an average. 
The harmonic mean, $H$, of the positive real numbers, $x_{1},~x_{2},~ \ldots,~x_{n}$ is defined as \\
\begin{equation*}
	\begin{aligned}
		H = \frac{n}{ \frac{1}{x_{1}} + \frac{1}{x_{2}} + \ldots + \frac{1}{x_{n}}} 
		& = \frac{n}{\sum_{i=1}^{n} \frac{1}{x_{i}}}
	\end{aligned}
\end{equation*}
The $F$ measure is the harmonic mean of precision and recall. 
Use this fact to derive Eq.(8.28) for $F$. 
In addition, write $F_{\beta}$ as a function of true positives, 
false negatives, and false positives.
\subsection*{Ans:}

\begin{gather*}
	F = \frac{2}{\frac{1}{precision} + \frac{1}{recall}} \\
	  = \frac{2 \times precision \times recall}{precision + recall} \\
\end{gather*}

\begin{gather*}
	F_{\beta} = \frac{(1 + \beta^2) \times TP}{(1 + \beta^2) \times TP + (\beta^2 \times FN) + FP}
\end{gather*}

\clearpage

\section*{Problem 3}
\textbf{9.4:} Compare the advantages and disadvantages of eager classification 
(e.g., decision tree, Bayesian, neural network) versus lazy classification 
(e.g., k-nearest neighbor).
\subsection*{Ans:}

\textbf{Eager classification} is faster at classification than lazy classification 
because it constructs a generalization model before receiving any new tuples to classify. 
Weights can be assigned to attributes, which can improve classification accuracy. 
Disadvantages of eager classification are that it must commit to a single hypothesis 
that covers the entire instance space, which can decrease classification, and more
time is needed for training.

\vspace{5ex}

\noindent \textbf{Lazy classification} uses a richer hypothesis space, which can improve classification 
accuracy. It requires less time for training than eager classification. A disadvantages 
of lazy classification is that all training tuples need to be stored, which leads to 
expensive storage costs and requires efficient indexing techniques. Another disadvantage 
is that it is slower at classification because classifiers are not built until new tuples 
need to be classified. Furthermore, attributes are all equally weighted, which can
decrease classification accuracy. (Problems may arise due to irrelevant attributes 
in the data.)

\vspace{5ex}

\noindent \textbf{Algorithm:} $k$-nearest neighbor. Build a $k$-nearest neighbor classifier.

\textbf{Input:}

\begin{itemize}
	
	\item Let $U$ be the unknown tuple whose class we want to assign.
	
	\item Let $T$ be the training set containing training tuples, 
	$T_{1} = (t_{1, 1}, t_{1, 2}, \ldots, t_{1, n}),\quad T_{2} = (t_{2, 1}, t_{2, 2}, \ldots, t_{2, n}), \ldots, \quad
	T_{m} = (t_{m, 1}, t_{m, 2}, \ldots, t_{m, n})$.

	\item Let attribute $t_{i, n}$ be the class label of $T_{i}$.
	
	\item Let $m$ be the number of training tuples.

	\item Let $n$ be the number of attributes describing each tuple.
	
	\item Let $k$ be the number of nearest neighbors we wish to find.

\end{itemize}

\textbf{Output:} Class label for $U$.

\clearpage

\textbf{Method:} The method is outlined as follows.

\begin{itemize}
	
	\item[1)] array $a[m][2]$; // $m$ rows containing data regarding the $m$ training tuples. The first column is the
	Euclidean distance between $U$ and that row’s training tuple. The second column refers to that
	training tuple’s index. We need to save the index because when sorting the array (according to
	Euclidean distance), we need some way to determine to which training set the Euclidean distance refers.

	\item[2)] \textbf{for} $i = 1$ to $m$ \textbf{do} \{

	\item[3)] $\quad a[i][1]$ = Euclidean distance($U, T_{i}$);
	
	\item[4)] $\quad a[i][2] = i$; \} // save the index, because rows will be sorted later
	
	\item[5)] Sort the rows of a by their Euclidean distances saved in $a[i][1]$ (in ascending order);
	
	\item[6)]  array $b[k][2]$; // The first column holds the distinct class labels of the $k$-nearest neighbors, while the
	second holds their respective counts. In the worst case, each $k$-nearest neighbor will have a different
	class label, which is why we need to allocate room for $k$ class labels.

	\item[7)] \textbf{for} $i = 1$ to $k$ \textbf{do} \{
	
	\item[8)] \quad \textbf{if} class label $t_{a[i][2]}$, $n$ already exists in array $b$ \textbf{then}
	
	\item[9)] \qquad find that class label’s row in array $b$ and increment its count;
	
	\item[10)] \quad \textbf{else} add the class label into the next available row of array $b$ and increment its count; \}
	
	\item[11)]  Sort array $b$ in descending order (from class label with largest count down to that with smallest count);
	
	\item[12)] \textbf{return}$(b[1])$; //return most frequent class label of the $k$-nearest neighbors of $U$ as the class prediction.

\end{itemize}

\end{document}
